{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Binary and Multinomial) Logistic Regression\n",
    "\n",
    "#### Generalized linear model\n",
    "- So far, all of our models are \"general linear models,\" and they are linear with homogenous variance.\n",
    "- Extending to \"generalized linear model,\" which relaxes the constraint of normality\n",
    "\n",
    "The scientists who induced generalized linear models have since apologized for the confusing name\n",
    "\n",
    "#### Specifics\n",
    "- Generalized linear models are seet up the same way as general linear models, but use what is called \"maximum likelihood\" to estimate regression coefficients, rather than least squares.\n",
    "- One example of a GLM is logistic regression—a model that is typically used for binary values or variables with three or more categories\n",
    "- Logistic regression interprets categorical values as probabilities of choosing those values, and then converts those probabilities into the odds of choosing one value or another (on a logarithmic scale)\n",
    "    - Conversion is known as a \"link function\"\n",
    "    - Depending on the data and the number of categories, logistic regression may assume a Bernoulli, binomial, categorical, or (ordered) multinomial distribution\n",
    "    \n",
    "#### Probit vs. logit models\n",
    "- Sometimes, when we talk about logistic regression, probit regression also comes up, since both regressions can handle binary DVs\n",
    "- While logit models converet probabilities of being in a category into the log odds of being in that category, probit models instead convert the probabilities into Z scores\n",
    "- The choice of link is situation dependent, but logistic regression tends to be more popular due to its easier interpretation.\n",
    " \n",
    "#### Binary logistic regression\n",
    "- Most basic logistic regression - two categories\n",
    "- Link function: `logit`\n",
    "- Our goal is to calculate the odds of being in one category or another.\n",
    "- To perform this in R, we can use thee `glm` function.\n",
    "\n",
    "e.g., predict whether sleep patterns have changed based on whether mood has worsened.\n",
    "\n",
    "$$ \\hat{Y}_{SleepPatternChanged} = b_0 + b_1 X_{HasMoodWorsened} $$\n",
    "\n",
    "(Assume dummy coding for this example)\n",
    "\n",
    "R code:\n",
    "`glm(SleepPatternChanged ~ HasMoodWorsened, family = binomial(), data = SHHWData`\n",
    "- `family` - distribution we expect the **DV** to follow\n",
    "\n",
    "In a normal regression (dummy coding):\n",
    "- `intercept` = mean of control\n",
    "- slope of dv = difference between mean of control and mean of treatment\n",
    "\n",
    "In a logistic regression (dummy coding):\n",
    "- log odds of sleep changing in control = intercept\n",
    "- log odds of sleep changing in treatment = dv slope\n",
    "\n",
    "Probability of YES in DV = `LogOddsIntercept` + `LogOddsVariable`\n",
    "\n",
    "#### Odds, logs, probabilities\n",
    "- Log odds to odds:\n",
    "$$ \\text{Odds} = e^{\\text{LogOdds}} $$\n",
    "- Odds to probabilities:\n",
    "$$ P = \\frac{O}{1+O} $$\n",
    "\n",
    "$$ logit^{-1}(x) = logistic(x) = \\frac{e^x}{1+e^x} $$\n",
    "\n",
    "#### Consideration\n",
    "- Don't backtransform until you've added the log-odds\n",
    "- In general, $L_1 + L_2 \\ne logit^{-1}(L_1) + logit^{-1}(L_2)$\n",
    "\n",
    "#### Logistic regression with continuous predictors\n",
    "- `intercept` = log odds of DV Yes at predictor = 0\n",
    "- dv slope = additional log odds of DV Yes for each additional value of predictor\n",
    "\n",
    "#### Multiple predictors\n",
    "- We can have multiple predictors\n",
    "\n",
    "#### Proportions\n",
    "- Note that the `glm` function will accept b inary values (0 or 1) or proportions (0.2, 0.5, etc.)\n",
    "- If you specify proportions, you'll need to specify the weight so that R can calculate the corresponding binary values\n",
    "\n",
    "#### Ordered multinomial logistic regression\n",
    "- We need to use a link that will accommodatee this ordering\n",
    "- Link: cumulative logit\n",
    "- Rather than the odds of ieing in a category, we calculate the odds of moving from one category into the next category over\n",
    "\n",
    "### Multiple regression\n",
    "Purposes of conducting regression:\n",
    "- Linear regression can be used in a confirmatory setting where a researcher wants to estimate the regression coefficients of a set of predictors.\n",
    "- Linear regression can also be used when it is not known *a priori* which predictors have an impact on the criterion.\n",
    "\n",
    "### Model selection\n",
    "- Best subset selection: out of fashion, but historically important\n",
    "- Can be used if you have $\\le 40$ predictors and want to know which and how many give the best predictions.\n",
    "\n",
    "Steps:\n",
    "1. Calculate residual SS for all possible combinations of predictors for all possible *k* (branch and bound algorithm)\n",
    "2. Figure out which *k* is the best\n",
    "\n",
    "#### Stepwise regression\n",
    "Start with zero predictors and iteratively add the next best predictor\n",
    "\n",
    "(Alternatively: start with all predictors, and delete the least important ones)\n",
    "\n",
    "At each step, the model needs to be re-estimated. Subsequent models are always nested.\n",
    "\n",
    "Potential drawback: the decision to include or exclude a predictor is final.\n",
    "\n",
    "You can also make the decision of feature selection using AIC/BIC stepwise.\n",
    "\n",
    "#### Example\n",
    "- Fit initial model to high number of predictors.\n",
    "    - Iteratively, drop one predictor and find smallest AIC/BIC\n",
    "    - When we stop getting gains from dropping, we are done.\n",
    "    \n",
    "#### Example 2\n",
    "- Fit initial model to a single predictor.\n",
    "    - Iteratively, add each predictor and see if it decreases the AIC/BIC.\n",
    "    - When we stop getting gains from adding, we're done.\n",
    "  \n",
    " \n",
    "**Hybrid model:** consider adding/dropping at each step\n",
    "\n",
    "#### Regularized (Penalized) Degression\n",
    "##### Shrinkage\n",
    "- In model selection approaches we've discussed so far, predictors are either in or out.\n",
    "    - The interpretation of significance is good, but sometimes the discreteness of inclusion/exclusion leads to inflated prediction error.\n",
    "- Shrinkage methods regulate the effects of predictors in a more continuous way. By placing a constraint on the sum of (a function of) the regression coefficients, they're shrunken towards zero and each other\n",
    "\n",
    "- Ridge and lasso regression differ by the type of constraint they impose\n",
    "\n",
    "##### Ridge regression\n",
    "\n",
    "Imposes constraint $\\sum^{p}_{j=1}{b^2_j} \\le t$, where t is a constant.\n",
    "\n",
    "When fitting a ridge regression, predictors are assumed to be normalized to Z, and Y is assumed to be centered. This is because ridge regression coefficients are not equivariant under scaling of predictors.\n",
    "\n",
    "- Ridge regression is a soft thresholding method.\n",
    "\n",
    "(In hard thresholding, we either keep or drop a preedictor. In soft thresholding, we simply shrink predictors)\n",
    "\n",
    "##### Lasso\n",
    "- The lasso has a different constraint: $\\sum^{p}_{j=1}{|b_j|} \\le t$\n",
    "- Ridge regression coefficients approach zero as the shrinkage parameter $\\lambda$ approaches infinity. Lasso shrinks coefficients to exactly zero at smaller values of $\\lambda$, so lasso can be use for variable selection.\n",
    "\n",
    "- Hard thresholding — does drop variables\n",
    "\n",
    "- Uses `glimmet` package in R\n",
    "\n",
    "#### Quantile regression\n",
    "- Quantile regression can be viewed as an extension of ordinary regression, but based on means\n",
    "- Motivation: typical regression only lets us view how some measure of central tendency varies with predictors\n",
    "    - We want to see how the distribution varies as a whole\n",
    "    \n",
    "Answer: fit a regression line to several percentiles of the dataset.\n",
    "- We can fit to any given percentile, including the median, which (barring extreme skew) is similar to the mean regression line\n",
    "\n",
    "(Cynthia shows a cool graph plotting the intercept against each quantile, and some linear interpolation between the points)\n",
    "\n",
    "#### Median/quantile regression advantages\n",
    "- More robust against outliers and nonnormal data.\n",
    "- Quantile regression provides a richer characterization of the data—we can see a set of conditional distributions, rather than a single conditional mean.\n",
    "\n",
    "#### Differences\n",
    "In median regression, we minimize the sum of absolute residuals rather than the sum of square residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
